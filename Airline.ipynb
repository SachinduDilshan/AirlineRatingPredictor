{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIokRF+HDtFBLu0bnqp915",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachinduDilshan/AirlineRatingPredictor/blob/main/Airline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE0hhbt1pNsp",
        "outputId": "4c9edaf7-9a11-49c0-f83c-1fbbecb5edd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text data...\n",
            "Extracting features...\n",
            "Splitting dataset...\n",
            "\n",
            "Training Multinomial Naive Bayes...\n",
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 0.7226775956284153\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.98      0.83      1791\n",
            "     neutral       0.72      0.29      0.41       648\n",
            "    positive       0.80      0.36      0.50       489\n",
            "\n",
            "    accuracy                           0.72      2928\n",
            "   macro avg       0.75      0.54      0.58      2928\n",
            "weighted avg       0.73      0.72      0.68      2928\n",
            "\n",
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Results:\n",
            "Accuracy: 0.7459016393442623\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.93      0.85      1791\n",
            "     neutral       0.62      0.41      0.49       648\n",
            "    positive       0.72      0.50      0.59       489\n",
            "\n",
            "    accuracy                           0.75      2928\n",
            "   macro avg       0.70      0.62      0.64      2928\n",
            "weighted avg       0.73      0.75      0.73      2928\n",
            "\n",
            "\n",
            "Example Predictions:\n",
            "Tweet: This airline is terrible, worst experience ever!\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Tweet: Great service and comfortable flight, thank you!\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Tweet: Flight was okay, nothing special\n",
            "Predicted Sentiment: negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Step 1: Load the Dataset\n",
        "# Assuming the file is named 'Tweets.csv'\n",
        "df = pd.read_csv('sample_data/Tweets.csv')\n",
        "df = df[[\"airline_sentiment\", \"text\"]]\n",
        "\n",
        "# Step 2: Preprocess Text\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Tokenize\n",
        "    text = nltk.word_tokenize(text)\n",
        "    # Remove stopwords and apply stemming\n",
        "    y = []\n",
        "    for i in text:\n",
        "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "            y.append(ps.stem(i))\n",
        "    return \" \".join(y)\n",
        "\n",
        "# Apply text cleaning\n",
        "print(\"Cleaning text data...\")\n",
        "df['text_cleaned'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Step 3: Feature Extraction\n",
        "print(\"Extracting features...\")\n",
        "# Create TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "\n",
        "# Generate TF-IDF vectors\n",
        "X = tfidf.fit_transform(df['text_cleaned']).toarray()\n",
        "Y = df['airline_sentiment'].values\n",
        "\n",
        "# Step 4: Train Models\n",
        "print(\"Splitting dataset...\")\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "print(\"\\nTraining Multinomial Naive Bayes...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "print(\"\\nNaive Bayes Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, nb_predictions))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "\n",
        "# Train Random Forest\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\nRandom Forest Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "# Function to predict sentiment for new tweets\n",
        "def predict_sentiment(new_text, model=rf_model):\n",
        "    \"\"\"\n",
        "    Predict sentiment for new tweets\n",
        "\n",
        "    Parameters:\n",
        "    new_text (str or list): New tweet(s) to analyze\n",
        "    model: Trained model to use (default: Random Forest)\n",
        "\n",
        "    Returns:\n",
        "    str or list: Predicted sentiment(s)\n",
        "    \"\"\"\n",
        "    # Clean the text\n",
        "    if isinstance(new_text, str):\n",
        "        cleaned = clean_text(new_text)\n",
        "        # Transform using the same vectorizer\n",
        "        vector = tfidf.transform([cleaned]).toarray()\n",
        "        return model.predict(vector)[0]\n",
        "    else:\n",
        "        cleaned = [clean_text(text) for text in new_text]\n",
        "        vectors = tfidf.transform(cleaned).toarray()\n",
        "        return model.predict(vectors)\n",
        "\n",
        "# Example usage of prediction function\n",
        "example_tweets = [\n",
        "    \"This airline is terrible, worst experience ever!\",\n",
        "    \"Great service and comfortable flight, thank you!\",\n",
        "    \"Flight was okay, nothing special\"\n",
        "]\n",
        "\n",
        "print(\"\\nExample Predictions:\")\n",
        "predictions = predict_sentiment(example_tweets)\n",
        "for tweet, pred in zip(example_tweets, predictions):\n",
        "    print(f\"Tweet: {tweet}\")\n",
        "    print(f\"Predicted Sentiment: {pred}\\n\")"
      ]
    }
  ]
}